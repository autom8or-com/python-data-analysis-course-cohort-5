{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK 8: ADVANCED FILTERING IN PYTHON - PART 3\n",
    "## Topic: Performance Optimization for Large Datasets\n",
    "## Business Case: Scaling to Production-Size E-Commerce Data\n",
    "\n",
    "---\n",
    "\n",
    "### LEARNING OBJECTIVES:\n",
    "1. Compare performance of different filtering approaches\n",
    "2. Use vectorized operations for speed\n",
    "3. Apply memory-efficient filtering techniques\n",
    "4. Leverage pandas optimization features\n",
    "5. Understand query optimization strategies\n",
    "6. Benchmark and profile filtering operations\n",
    "\n",
    "### BUSINESS CONTEXT:\n",
    "When working with production datasets (millions of rows), performance matters! Learn to optimize your filtering code for speed and memory efficiency - essential for real-world analytics.\n",
    "\n",
    "### PERFORMANCE HIERARCHY (Fastest to Slowest):\n",
    "1. ‚úÖ Vectorized operations (NumPy/pandas)\n",
    "2. ‚úÖ `.query()` method\n",
    "3. ‚úÖ Boolean indexing with single mask\n",
    "4. ‚ö†Ô∏è `.apply()` with lambda\n",
    "5. ‚ùå Python loops (AVOID!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Libraries imported successfully!')\n",
    "print(f'Pandas version: {pd.__version__}')\n",
    "print(f'NumPy version: {np.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Create Large Sample Dataset\n",
    "\n",
    "We'll simulate a large e-commerce dataset to test performance optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate large sample dataset (100,000 records)\n",
    "np.random.seed(42)\n",
    "n_rows = 100000\n",
    "\n",
    "print(f\"Generating dataset with {n_rows:,} rows...\")\n",
    "\n",
    "large_dataset = pd.DataFrame({\n",
    "    'order_id': [f'order_{i:06d}' for i in range(n_rows)],\n",
    "    'customer_id': np.random.choice([f'cust_{i:05d}' for i in range(10000)], n_rows),\n",
    "    'customer_state': np.random.choice(\n",
    "        ['SP', 'RJ', 'MG', 'BA', 'PE', 'RS', 'PR', 'SC', 'CE', 'GO'], \n",
    "        n_rows, \n",
    "        p=[0.35, 0.2, 0.15, 0.08, 0.06, 0.05, 0.04, 0.03, 0.02, 0.02]\n",
    "    ),\n",
    "    'order_value': np.random.uniform(10, 1000, n_rows).round(2),\n",
    "    'freight_value': np.random.uniform(5, 100, n_rows).round(2),\n",
    "    'review_score': np.random.choice(\n",
    "        [1.0, 2.0, 3.0, 4.0, 5.0, np.nan], \n",
    "        n_rows, \n",
    "        p=[0.05, 0.1, 0.15, 0.3, 0.35, 0.05]\n",
    "    ),\n",
    "    'order_status': np.random.choice(\n",
    "        ['delivered', 'shipped', 'canceled', 'processing'], \n",
    "        n_rows, \n",
    "        p=[0.75, 0.15, 0.05, 0.05]\n",
    "    ),\n",
    "    'payment_type': np.random.choice(\n",
    "        ['credit_card', 'boleto', 'voucher', 'debit_card'], \n",
    "        n_rows, \n",
    "        p=[0.6, 0.25, 0.1, 0.05]\n",
    "    ),\n",
    "    'payment_installments': np.random.choice([1, 2, 3, 4, 6, 10, 12], n_rows)\n",
    "})\n",
    "\n",
    "# Add calculated columns\n",
    "large_dataset['total_value'] = large_dataset['order_value'] + large_dataset['freight_value']\n",
    "large_dataset['order_date'] = pd.date_range('2017-01-01', periods=n_rows, freq='5min')\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset created: {len(large_dataset):,} rows\")\n",
    "print(f\"Memory usage: {large_dataset.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(large_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset info\n",
    "print(\"Dataset Information:\")\n",
    "print(large_dataset.info())\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(large_dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Performance Comparison - Filtering Methods\n",
    "\n",
    "Let's benchmark different filtering approaches on our large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create benchmark utility function\n",
    "def benchmark_filter(df, method_name, filter_func, runs=5):\n",
    "    \"\"\"\n",
    "    Benchmark a filtering function.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to filter\n",
    "    - method_name: Name of the method for reporting\n",
    "    - filter_func: Function that takes df and returns filtered df\n",
    "    - runs: Number of times to run for averaging\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    result_size = 0\n",
    "    \n",
    "    for i in range(runs):\n",
    "        start_time = time.perf_counter()\n",
    "        result = filter_func(df)\n",
    "        end_time = time.perf_counter()\n",
    "        times.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
    "        result_size = len(result)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    return {\n",
    "        'Method': method_name,\n",
    "        'Avg Time (ms)': round(avg_time, 2),\n",
    "        'Std Dev (ms)': round(std_time, 2),\n",
    "        'Result Rows': result_size\n",
    "    }\n",
    "\n",
    "print(\"Benchmark function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case: High-Value Orders from Major States with Good Reviews\n",
    "\n",
    "Filter criteria:\n",
    "- Total value > R$ 200\n",
    "- Customer state in ['SP', 'RJ', 'MG']\n",
    "- Review score >= 4\n",
    "- Order status = 'delivered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filtering methods\n",
    "benchmark_results = []\n",
    "\n",
    "# Method 1: Boolean indexing (standard approach)\n",
    "def method1_boolean_indexing(df):\n",
    "    return df[\n",
    "        (df['total_value'] > 200) &\n",
    "        (df['customer_state'].isin(['SP', 'RJ', 'MG'])) &\n",
    "        (df['review_score'] >= 4) &\n",
    "        (df['order_status'] == 'delivered')\n",
    "    ]\n",
    "\n",
    "# Method 2: .query() method\n",
    "def method2_query(df):\n",
    "    return df.query(\n",
    "        \"total_value > 200 and \"\n",
    "        \"customer_state in ['SP', 'RJ', 'MG'] and \"\n",
    "        \"review_score >= 4 and \"\n",
    "        \"order_status == 'delivered'\"\n",
    "    )\n",
    "\n",
    "# Method 3: Multiple sequential filters (INEFFICIENT)\n",
    "def method3_sequential(df):\n",
    "    temp1 = df[df['total_value'] > 200]\n",
    "    temp2 = temp1[temp1['customer_state'].isin(['SP', 'RJ', 'MG'])]\n",
    "    temp3 = temp2[temp2['review_score'] >= 4]\n",
    "    return temp3[temp3['order_status'] == 'delivered']\n",
    "\n",
    "# Method 4: Using .loc with boolean mask\n",
    "def method4_loc(df):\n",
    "    mask = (\n",
    "        (df['total_value'] > 200) &\n",
    "        (df['customer_state'].isin(['SP', 'RJ', 'MG'])) &\n",
    "        (df['review_score'] >= 4) &\n",
    "        (df['order_status'] == 'delivered')\n",
    "    )\n",
    "    return df.loc[mask]\n",
    "\n",
    "# Method 5: Filter in stages (optimized order)\n",
    "def method5_staged(df):\n",
    "    # Filter most selective first\n",
    "    df_high_value = df[df['total_value'] > 200]\n",
    "    df_delivered = df_high_value[df_high_value['order_status'] == 'delivered']\n",
    "    df_states = df_delivered[df_delivered['customer_state'].isin(['SP', 'RJ', 'MG'])]\n",
    "    return df_states[df_states['review_score'] >= 4]\n",
    "\n",
    "# Run all benchmarks\n",
    "print(\"Running benchmarks... (this may take a moment)\\n\")\n",
    "benchmark_results.append(benchmark_filter(large_dataset, 'Boolean Indexing', method1_boolean_indexing))\n",
    "benchmark_results.append(benchmark_filter(large_dataset, '.query() Method', method2_query))\n",
    "benchmark_results.append(benchmark_filter(large_dataset, 'Sequential Filters', method3_sequential))\n",
    "benchmark_results.append(benchmark_filter(large_dataset, '.loc with Mask', method4_loc))\n",
    "benchmark_results.append(benchmark_filter(large_dataset, 'Staged Filtering', method5_staged))\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(benchmark_results).sort_values('Avg Time (ms)')\n",
    "results_df['Relative Speed'] = (results_df['Avg Time (ms)'].min() / results_df['Avg Time (ms)']).round(2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON - FILTERING METHODS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úÖ Fastest Method: {results_df.iloc[0]['Method']}\")\n",
    "print(f\"‚è±Ô∏è  Time: {results_df.iloc[0]['Avg Time (ms)']} ms\")\n",
    "print(f\"üìä Results: {results_df.iloc[0]['Result Rows']:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Memory-Efficient Filtering\n",
    "\n",
    "Reducing memory usage is crucial for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare memory usage of different approaches\n",
    "import sys\n",
    "\n",
    "# Memory-inefficient: Creating multiple intermediate dataframes\n",
    "def memory_check_inefficient(df):\n",
    "    step1 = df[df['total_value'] > 200]  # Creates copy\n",
    "    step2 = step1[step1['customer_state'].isin(['SP', 'RJ'])]  # Creates another copy\n",
    "    step3 = step2[step2['review_score'] >= 4]  # Creates another copy\n",
    "    return step3\n",
    "\n",
    "# Memory-efficient: Single boolean mask\n",
    "def memory_check_efficient(df):\n",
    "    mask = (\n",
    "        (df['total_value'] > 200) &\n",
    "        (df['customer_state'].isin(['SP', 'RJ'])) &\n",
    "        (df['review_score'] >= 4)\n",
    "    )\n",
    "    return df[mask]  # Creates one copy\n",
    "\n",
    "print(\"Memory Usage Analysis:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original dataset: {large_dataset.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nInefficient approach:\")\n",
    "print(\"  - Creates multiple intermediate DataFrames\")\n",
    "print(\"  - Each intermediate copy uses memory\")\n",
    "print(\"  - Memory spikes during execution\")\n",
    "print(\"\\nEfficient approach:\")\n",
    "print(\"  - Creates single boolean mask (very small)\")\n",
    "print(\"  - Only one final DataFrame copy\")\n",
    "print(\"  - Minimal memory overhead\")\n",
    "print(\"\\n‚úÖ Best Practice: Use single combined boolean mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Data Type Optimization\n",
    "\n",
    "Optimizing data types can significantly reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current memory usage\n",
    "print(\"ORIGINAL DATA TYPES AND MEMORY USAGE\")\n",
    "print(\"=\"*60)\n",
    "print(large_dataset.dtypes)\n",
    "print(f\"\\nTotal memory: {large_dataset.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Create optimized version\n",
    "optimized_dataset = large_dataset.copy()\n",
    "\n",
    "# Convert strings to categories (huge memory savings!)\n",
    "optimized_dataset['customer_state'] = optimized_dataset['customer_state'].astype('category')\n",
    "optimized_dataset['order_status'] = optimized_dataset['order_status'].astype('category')\n",
    "optimized_dataset['payment_type'] = optimized_dataset['payment_type'].astype('category')\n",
    "\n",
    "# Convert float64 to float32 where precision isn't critical\n",
    "optimized_dataset['review_score'] = optimized_dataset['review_score'].astype('float32')\n",
    "optimized_dataset['order_value'] = optimized_dataset['order_value'].astype('float32')\n",
    "optimized_dataset['freight_value'] = optimized_dataset['freight_value'].astype('float32')\n",
    "optimized_dataset['total_value'] = optimized_dataset['total_value'].astype('float32')\n",
    "\n",
    "# Convert int64 to int32 or int16 where appropriate\n",
    "optimized_dataset['payment_installments'] = optimized_dataset['payment_installments'].astype('int8')\n",
    "\n",
    "print(\"\\n\\nOPTIMIZED DATA TYPES AND MEMORY USAGE\")\n",
    "print(\"=\"*60)\n",
    "print(optimized_dataset.dtypes)\n",
    "print(f\"\\nTotal memory: {optimized_dataset.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Calculate savings\n",
    "original_memory = large_dataset.memory_usage(deep=True).sum() / 1024**2\n",
    "optimized_memory = optimized_dataset.memory_usage(deep=True).sum() / 1024**2\n",
    "savings = (1 - optimized_memory / original_memory) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üíæ Memory saved: {savings:.1f}%\")\n",
    "print(f\"‚úÖ Reduction: {original_memory - optimized_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Vectorized Operations vs Apply\n",
    "\n",
    "Demonstrating why vectorized operations are dramatically faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset for demonstration (apply is VERY slow on large data)\n",
    "demo_data = large_dataset.head(10000).copy()\n",
    "\n",
    "# Task: Categorize orders by value\n",
    "# BAD: Using .apply() with lambda\n",
    "start = time.perf_counter()\n",
    "demo_data['category_apply'] = demo_data['total_value'].apply(\n",
    "    lambda x: 'High' if x > 300 else ('Medium' if x > 150 else 'Low')\n",
    ")\n",
    "apply_time = (time.perf_counter() - start) * 1000\n",
    "\n",
    "# GOOD: Using vectorized operations (pd.cut or np.where)\n",
    "start = time.perf_counter()\n",
    "demo_data['category_vectorized'] = pd.cut(\n",
    "    demo_data['total_value'],\n",
    "    bins=[0, 150, 300, float('inf')],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "vectorized_time = (time.perf_counter() - start) * 1000\n",
    "\n",
    "# Alternative: Using np.select (also fast)\n",
    "start = time.perf_counter()\n",
    "conditions = [\n",
    "    demo_data['total_value'] > 300,\n",
    "    demo_data['total_value'] > 150,\n",
    "    demo_data['total_value'] <= 150\n",
    "]\n",
    "choices = ['High', 'Medium', 'Low']\n",
    "demo_data['category_select'] = np.select(conditions, choices)\n",
    "select_time = (time.perf_counter() - start) * 1000\n",
    "\n",
    "print(\"VECTORIZATION vs APPLY PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset size: {len(demo_data):,} rows\")\n",
    "print(\"\\nTask: Categorize orders by total value\\n\")\n",
    "print(f\".apply() with lambda:  {apply_time:.2f} ms\")\n",
    "print(f\"pd.cut() vectorized:   {vectorized_time:.2f} ms  ({apply_time/vectorized_time:.1f}x FASTER)\")\n",
    "print(f\"np.select() vectorized: {select_time:.2f} ms  ({apply_time/select_time:.1f}x FASTER)\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚ö†Ô∏è  WARNING: .apply() can be 10-100x slower than vectorized operations!\")\n",
    "print(\"‚úÖ ALWAYS try to vectorize your operations instead of using .apply()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Filter Order Optimization\n",
    "\n",
    "The order in which you apply filters can impact performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy: Filter most selective conditions FIRST\n",
    "\n",
    "# Check selectivity of each condition\n",
    "print(\"FILTER SELECTIVITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "total_rows = len(large_dataset)\n",
    "\n",
    "filters = [\n",
    "    ('total_value > 500', large_dataset['total_value'] > 500),\n",
    "    ('review_score >= 4', large_dataset['review_score'] >= 4),\n",
    "    ('customer_state == SP', large_dataset['customer_state'] == 'SP'),\n",
    "    ('order_status == delivered', large_dataset['order_status'] == 'delivered')\n",
    "]\n",
    "\n",
    "selectivity = []\n",
    "for name, condition in filters:\n",
    "    rows_remaining = condition.sum()\n",
    "    selectivity_pct = (rows_remaining / total_rows) * 100\n",
    "    selectivity.append({\n",
    "        'Filter': name,\n",
    "        'Rows Remaining': f\"{rows_remaining:,}\",\n",
    "        'Selectivity %': f\"{selectivity_pct:.1f}%\"\n",
    "    })\n",
    "\n",
    "selectivity_df = pd.DataFrame(selectivity)\n",
    "print(selectivity_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° TIP: Apply most selective filters (lowest %) first!\")\n",
    "print(\"   This reduces the data size early in the pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Production-Ready Customer Segmentation Pipeline\n",
    "\n",
    "Putting it all together: Optimized, fast, memory-efficient customer analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_customer_segmentation(df, value_threshold=200, review_threshold=4):\n",
    "    \"\"\"\n",
    "    High-performance customer segmentation using best practices.\n",
    "    \n",
    "    Returns: Dictionary of customer segments\n",
    "    \"\"\"\n",
    "    # Use optimized dataset if available\n",
    "    if df['customer_state'].dtype.name != 'category':\n",
    "        print(\"‚ö†Ô∏è  Warning: Dataset not optimized. Consider optimizing dtypes first.\")\n",
    "    \n",
    "    # Segment 1: Champions (high value + high satisfaction)\n",
    "    champions = df.query(\n",
    "        'total_value >= @value_threshold and '\n",
    "        'review_score >= @review_threshold and '\n",
    "        'order_status == \"delivered\"'\n",
    "    )\n",
    "    \n",
    "    # Segment 2: At Risk (high value but low satisfaction)\n",
    "    at_risk = df.query(\n",
    "        'total_value >= @value_threshold and '\n",
    "        'review_score < 3 and '\n",
    "        'review_score.notna() and '\n",
    "        'order_status == \"delivered\"'\n",
    "    )\n",
    "    \n",
    "    # Segment 3: Potential (medium value, good satisfaction)\n",
    "    potential = df.query(\n",
    "        'total_value >= 100 and total_value < @value_threshold and '\n",
    "        'review_score >= @review_threshold and '\n",
    "        'order_status == \"delivered\"'\n",
    "    )\n",
    "    \n",
    "    # Segment 4: Lost (canceled orders)\n",
    "    lost = df.query('order_status == \"canceled\"')\n",
    "    \n",
    "    return {\n",
    "        'champions': champions,\n",
    "        'at_risk': at_risk,\n",
    "        'potential': potential,\n",
    "        'lost': lost\n",
    "    }\n",
    "\n",
    "# Run optimized segmentation\n",
    "print(\"Running optimized customer segmentation...\\n\")\n",
    "start = time.perf_counter()\n",
    "segments = optimized_customer_segmentation(optimized_dataset)\n",
    "execution_time = (time.perf_counter() - start) * 1000\n",
    "\n",
    "# Generate summary report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CUSTOMER SEGMENTATION DASHBOARD\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚è±Ô∏è  Execution time: {execution_time:.2f} ms\")\n",
    "print(f\"üìä Total orders analyzed: {len(optimized_dataset):,}\\n\")\n",
    "\n",
    "summary_data = []\n",
    "for segment_name, segment_df in segments.items():\n",
    "    summary_data.append({\n",
    "        'Segment': segment_name.title(),\n",
    "        'Customers': len(segment_df),\n",
    "        'Total Revenue': f\"R$ {segment_df['total_value'].sum():,.2f}\",\n",
    "        'Avg Order Value': f\"R$ {segment_df['total_value'].mean():.2f}\" if len(segment_df) > 0 else 'N/A',\n",
    "        'Avg Review': f\"{segment_df['review_score'].mean():.2f}\" if segment_df['review_score'].notna().any() else 'N/A'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n‚úÖ Segmentation complete and optimized for production use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Performance Best Practices Summary\n",
    "\n",
    "### PERFORMANCE HIERARCHY (Fast ‚Üí Slow):\n",
    "\n",
    "1. **üöÄ Vectorized Operations** (FASTEST)\n",
    "   - NumPy/pandas built-in functions\n",
    "   - Example: `df['col'] > threshold`\n",
    "   - **10-100x faster than loops**\n",
    "\n",
    "2. **‚ö° `.query()` Method**\n",
    "   - Good for complex multi-condition filters\n",
    "   - Readable SQL-like syntax\n",
    "   - Often comparable to boolean indexing\n",
    "\n",
    "3. **‚úÖ Boolean Indexing with Single Mask**\n",
    "   - Combine conditions in one expression\n",
    "   - Use `.loc[]` for explicit indexing\n",
    "   - Memory efficient\n",
    "\n",
    "4. **‚ö†Ô∏è `.apply()` with Lambda** (SLOWER)\n",
    "   - Avoid when possible\n",
    "   - Use only when vectorization is impossible\n",
    "   - 10-50x slower than vectorized operations\n",
    "\n",
    "5. **‚ùå Python Loops** (SLOWEST - AVOID!)\n",
    "   - Never iterate row-by-row\n",
    "   - Always find vectorized alternative\n",
    "   - **100x+ slower than vectorized**\n",
    "\n",
    "### OPTIMIZATION CHECKLIST:\n",
    "\n",
    "#### Data Type Optimization:\n",
    "‚úÖ Convert categorical strings to `category` dtype\n",
    "‚úÖ Use `float32` instead of `float64` where appropriate\n",
    "‚úÖ Use smallest integer type that fits your data\n",
    "‚úÖ Can save 50-80% memory\n",
    "\n",
    "#### Filtering Optimization:\n",
    "‚úÖ Filter early (reduce data size ASAP)\n",
    "‚úÖ Create single boolean mask (not multiple)\n",
    "‚úÖ Apply most selective filters first\n",
    "‚úÖ Use `.query()` for complex conditions\n",
    "\n",
    "#### Operation Optimization:\n",
    "‚úÖ Use vectorized operations always\n",
    "‚úÖ Avoid `.apply()` when possible\n",
    "‚úÖ Never use Python loops on large data\n",
    "‚úÖ Use `pd.cut()`, `np.select()`, `np.where()` for categorization\n",
    "\n",
    "#### Memory Management:\n",
    "‚úÖ Avoid unnecessary `.copy()` operations\n",
    "‚úÖ Delete intermediate DataFrames not needed\n",
    "‚úÖ Consider chunking for very large datasets\n",
    "‚úÖ Monitor memory usage with `.memory_usage(deep=True)`\n",
    "\n",
    "#### Development Workflow:\n",
    "‚úÖ Always benchmark different approaches\n",
    "‚úÖ Profile code to find bottlenecks\n",
    "‚úÖ Test with production-size data\n",
    "‚úÖ Document performance considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Performance:\n",
    "1. **Vectorized operations are 10-100x faster** than loops/apply\n",
    "2. `.query()` provides good balance of speed and readability\n",
    "3. Filter early and with most selective conditions first\n",
    "4. Single boolean mask is more efficient than multiple filters\n",
    "\n",
    "### Memory:\n",
    "1. Data type optimization can save **50-80% memory**\n",
    "2. Category dtype for string columns is essential\n",
    "3. Avoid creating unnecessary intermediate DataFrames\n",
    "4. Single boolean mask uses minimal memory\n",
    "\n",
    "### Production Tips:\n",
    "1. Always benchmark with production-size data\n",
    "2. Profile to identify actual bottlenecks (don't guess!)\n",
    "3. Document performance characteristics\n",
    "4. Set up monitoring for long-running processes\n",
    "\n",
    "---\n",
    "\n",
    "## Week 8 Python Series Complete!\n",
    "\n",
    "You've now mastered:\n",
    "- **Part 1:** Complex boolean filtering (`&`, `|`, `~`, `.isin()`)\n",
    "- **Part 2:** SQL-like filtering with `.query()`\n",
    "- **Part 3:** Performance optimization for production datasets\n",
    "\n",
    "### What's Next?\n",
    "Practice these techniques on the Week 8 exercises using real Olist e-commerce data!\n",
    "\n",
    "### Real-World Application:\n",
    "These optimization techniques are essential for:\n",
    "- Production data pipelines\n",
    "- Real-time analytics dashboards\n",
    "- Large-scale customer analysis\n",
    "- E-commerce recommendation systems\n",
    "- Business intelligence reporting\n",
    "\n",
    "**Remember:** Premature optimization is the root of all evil, but optimization for production scale is essential! Always measure before and after."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
