{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Date & Time Operations - Part 1: Python DateTime Fundamentals\n",
    "**Business Scenario**: NaijaCommerce Seasonal Analysis\n",
    "**Data Source**: Brazilian Olist E-commerce Dataset\n",
    "\n",
    "## Learning Objectives\n",
    "- Convert strings to datetime objects using pandas and datetime libraries\n",
    "- Extract meaningful date components for temporal analysis\n",
    "- Handle missing and malformed date data\n",
    "- Prepare temporal data for business analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Import Required Libraries\n",
    "Let's start by importing all the libraries we'll need for our temporal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Date and time handling\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ğŸ“¦ All libraries imported successfully!\")\n",
    "print(f\"ğŸ“Š Pandas version: {pd.__version__}\")\n",
    "print(f\"ğŸ Python datetime module loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‚ Loading the Olist E-commerce Dataset\n",
    "We'll work with the same Brazilian e-commerce data that we analyzed with SQL on Thursday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the orders dataset\n",
    "# Note: In a real scenario, you would download the Olist dataset from Kaggle\n",
    "# For this example, we'll create sample data that mirrors the SQL structure\n",
    "\n",
    "# For demonstration, let's create sample data similar to our SQL dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate sample order data\n",
    "n_orders = 10000\n",
    "start_date = datetime(2016, 9, 1)\n",
    "end_date = datetime(2018, 10, 31)\n",
    "\n",
    "# Create sample dataset\n",
    "orders_df = pd.DataFrame({\n",
    "    'order_id': [f'order_{i:06d}' for i in range(n_orders)],\n",
    "    'customer_id': [f'customer_{np.random.randint(1, 5000):06d}' for _ in range(n_orders)],\n",
    "    'order_purchase_timestamp': pd.date_range(start=start_date, end=end_date, periods=n_orders),\n",
    "    'order_status': np.random.choice(['delivered', 'shipped', 'cancelled'], n_orders, p=[0.8, 0.15, 0.05])\n",
    "})\n",
    "\n",
    "# Add delivery timestamps with realistic delays\n",
    "delivery_delays = np.random.exponential(scale=7, size=n_orders)  # Average 7 days\n",
    "orders_df['order_delivered_customer_date'] = orders_df['order_purchase_timestamp'] + pd.to_timedelta(delivery_delays, unit='D')\n",
    "\n",
    "# Add approval timestamps (usually 1-3 days after purchase)\n",
    "approval_delays = np.random.exponential(scale=1.5, size=n_orders)  # Average 1.5 days\n",
    "orders_df['order_approved_at'] = orders_df['order_purchase_timestamp'] + pd.to_timedelta(approval_delays, unit='D')\n",
    "\n",
    "# Set delivery dates to None for non-delivered orders\n",
    "mask = orders_df['order_status'] != 'delivered'\n",
    "orders_df.loc[mask, 'order_delivered_customer_date'] = pd.NaT\n",
    "\n",
    "print(f\"ğŸ“Š Created sample dataset with {len(orders_df):,} orders\")\n",
    "print(f\"ğŸ“… Date range: {orders_df['order_purchase_timestamp'].min()} to {orders_df['order_purchase_timestamp'].max()}\")\n",
    "orders_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Part 1: Understanding Python Date/Time Objects\n",
    "Let's explore the different types of datetime objects and how to work with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Python datetime vs Pandas datetime64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python's native datetime\n",
    "python_datetime = datetime(2024, 12, 25, 14, 30, 0)\n",
    "print(f\"Python datetime: {python_datetime}\")\n",
    "print(f\"Type: {type(python_datetime)}\")\n",
    "\n",
    "# Pandas datetime (numpy datetime64)\n",
    "pandas_datetime = pd.Timestamp('2024-12-25 14:30:00')\n",
    "print(f\"\\nPandas datetime: {pandas_datetime}\")\n",
    "print(f\"Type: {type(pandas_datetime)}\")\n",
    "\n",
    "# Check our dataset's datetime column type\n",
    "print(f\"\\nOur order timestamp type: {type(orders_df['order_purchase_timestamp'].iloc[0])}\")\n",
    "print(f\"Column dtype: {orders_df['order_purchase_timestamp'].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Converting Strings to DateTime Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common date string formats and how to parse them\n",
    "date_strings = [\n",
    "    '2024-12-25',           # ISO format\n",
    "    '25/12/2024',           # European format\n",
    "    'Dec 25, 2024',         # US readable format\n",
    "    '2024-12-25 14:30:00',  # ISO with time\n",
    "    '25-12-2024 2:30 PM'    # Mixed format\n",
    "]\n",
    "\n",
    "# Using pd.to_datetime() with different formats\n",
    "print(\"ğŸ”„ Converting various date string formats:\")\n",
    "for date_str in date_strings:\n",
    "    try:\n",
    "        # pd.to_datetime() is very flexible and can handle many formats automatically\n",
    "        converted = pd.to_datetime(date_str)\n",
    "        print(f\"'{date_str}' â†’ {converted}\")\n",
    "    except Exception as e:\n",
    "        print(f\"'{date_str}' â†’ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling specific formats with explicit format strings\n",
    "# This is faster when you know the exact format\n",
    "\n",
    "date_str = '25-12-2024 14:30:00'\n",
    "print(f\"Original string: '{date_str}'\")\n",
    "\n",
    "# Using format specification (faster for large datasets)\n",
    "parsed_with_format = pd.to_datetime(date_str, format='%d-%m-%Y %H:%M:%S')\n",
    "print(f\"Parsed with format: {parsed_with_format}\")\n",
    "\n",
    "# Let pd.to_datetime infer the format (slower but more flexible)\n",
    "parsed_inferred = pd.to_datetime(date_str)\n",
    "print(f\"Parsed with inference: {parsed_inferred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Handling Missing and Malformed Date Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with missing and malformed dates\n",
    "problematic_dates = pd.Series([\n",
    "    '2024-12-25',\n",
    "    'invalid_date',\n",
    "    '',\n",
    "    None,\n",
    "    '2024-13-45',  # Invalid month and day\n",
    "    '2024-12-31',\n",
    "    np.nan\n",
    "])\n",
    "\n",
    "print(\"ğŸš¨ Handling problematic date data:\")\n",
    "print(\"Original data:\")\n",
    "print(problematic_dates)\n",
    "\n",
    "# Convert with error handling\n",
    "# errors='coerce' converts invalid dates to NaT (Not a Time)\n",
    "converted_dates = pd.to_datetime(problematic_dates, errors='coerce')\n",
    "print(\"\\nAfter conversion (errors='coerce'):\")\n",
    "print(converted_dates)\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nğŸ“Š Missing values: {converted_dates.isna().sum()}\")\n",
    "print(f\"ğŸ“Š Valid dates: {converted_dates.notna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Part 2: Pandas DateTime Accessor (.dt)\n",
    "The `.dt` accessor is your gateway to extracting date components and performing temporal operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extracting Date Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract various date components from our orders data\n",
    "# This is equivalent to SQL's EXTRACT() function\n",
    "\n",
    "# Create a smaller sample for clear demonstration\n",
    "sample_orders = orders_df.head(10).copy()\n",
    "\n",
    "# Extract date components\n",
    "sample_orders['purchase_year'] = sample_orders['order_purchase_timestamp'].dt.year\n",
    "sample_orders['purchase_month'] = sample_orders['order_purchase_timestamp'].dt.month\n",
    "sample_orders['purchase_quarter'] = sample_orders['order_purchase_timestamp'].dt.quarter\n",
    "sample_orders['purchase_day'] = sample_orders['order_purchase_timestamp'].dt.day\n",
    "sample_orders['purchase_dayofweek'] = sample_orders['order_purchase_timestamp'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "sample_orders['purchase_hour'] = sample_orders['order_purchase_timestamp'].dt.hour\n",
    "\n",
    "# Display the results\n",
    "columns_to_show = ['order_id', 'order_purchase_timestamp', 'purchase_year', \n",
    "                   'purchase_month', 'purchase_quarter', 'purchase_dayofweek']\n",
    "print(\"ğŸ“… Date component extraction (similar to SQL EXTRACT function):\")\n",
    "print(sample_orders[columns_to_show])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create business-friendly date formats\n",
    "# This is equivalent to SQL's TO_CHAR() function\n",
    "\n",
    "sample_orders['month_name'] = sample_orders['order_purchase_timestamp'].dt.month_name()\n",
    "sample_orders['day_name'] = sample_orders['order_purchase_timestamp'].dt.day_name()\n",
    "sample_orders['date_only'] = sample_orders['order_purchase_timestamp'].dt.date\n",
    "sample_orders['time_only'] = sample_orders['order_purchase_timestamp'].dt.time\n",
    "sample_orders['month_year'] = sample_orders['order_purchase_timestamp'].dt.strftime('%B %Y')\n",
    "\n",
    "# Display formatted dates\n",
    "format_columns = ['order_id', 'order_purchase_timestamp', 'month_name', 'day_name', 'month_year']\n",
    "print(\"ğŸ“ Business-friendly date formatting:\")\n",
    "print(sample_orders[format_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Business Application: Monthly Sales Summary\n",
    "Let's replicate the SQL analysis we did on Thursday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly sales summary - Python equivalent of our SQL GROUP BY analysis\n",
    "monthly_sales = orders_df.groupby([\n",
    "    orders_df['order_purchase_timestamp'].dt.year,\n",
    "    orders_df['order_purchase_timestamp'].dt.month\n",
    "]).agg({\n",
    "    'order_id': 'count',\n",
    "    'customer_id': 'nunique'\n",
    "}).rename(columns={\n",
    "    'order_id': 'total_orders',\n",
    "    'customer_id': 'unique_customers'\n",
    "})\n",
    "\n",
    "# Reset index to make year and month regular columns\n",
    "monthly_sales = monthly_sales.reset_index()\n",
    "monthly_sales.columns = ['year', 'month', 'total_orders', 'unique_customers']\n",
    "\n",
    "# Add month names for readability\n",
    "monthly_sales['month_name'] = pd.to_datetime(monthly_sales[['year', 'month']].assign(day=1)).dt.month_name()\n",
    "\n",
    "print(\"ğŸ“Š Monthly Sales Summary (equivalent to our Thursday SQL analysis):\")\n",
    "print(monthly_sales.head(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Weekend vs Weekday Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze shopping patterns by day type\n",
    "# Python equivalent of our SQL weekend/weekday analysis\n",
    "\n",
    "# Add day type classification\n",
    "orders_df['day_of_week'] = orders_df['order_purchase_timestamp'].dt.dayofweek\n",
    "orders_df['day_type'] = orders_df['day_of_week'].apply(\n",
    "    lambda x: 'Weekend' if x >= 5 else 'Weekday'  # 5=Saturday, 6=Sunday\n",
    ")\n",
    "\n",
    "# Calculate summary by day type\n",
    "day_type_summary = orders_df.groupby('day_type').agg({\n",
    "    'order_id': 'count'\n",
    "}).rename(columns={'order_id': 'total_orders'})\n",
    "\n",
    "# Calculate percentages\n",
    "day_type_summary['percentage_of_total'] = (\n",
    "    day_type_summary['total_orders'] / day_type_summary['total_orders'].sum() * 100\n",
    ").round(2)\n",
    "\n",
    "print(\"ğŸ“… Weekend vs Weekday Shopping Patterns:\")\n",
    "print(day_type_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Part 3: Time-Based Grouping and Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Using resample() for Time-Based Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set datetime as index for resampling\n",
    "orders_indexed = orders_df.set_index('order_purchase_timestamp').sort_index()\n",
    "\n",
    "# Monthly resampling (equivalent to SQL DATE_TRUNC('month', date))\n",
    "monthly_orders = orders_indexed.resample('M').agg({\n",
    "    'order_id': 'count',\n",
    "    'customer_id': 'nunique'\n",
    "}).rename(columns={\n",
    "    'order_id': 'monthly_orders',\n",
    "    'customer_id': 'monthly_customers'\n",
    "})\n",
    "\n",
    "print(\"ğŸ“Š Monthly Order Trends (using resample):\")\n",
    "print(monthly_orders.head(10))\n",
    "\n",
    "# Weekly resampling\n",
    "weekly_orders = orders_indexed.resample('W').agg({\n",
    "    'order_id': 'count'\n",
    "}).rename(columns={'order_id': 'weekly_orders'})\n",
    "\n",
    "print(\"\\nğŸ“Š Weekly Order Trends:\")\n",
    "print(weekly_orders.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Peak Shopping Hours Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze shopping patterns by hour of day\n",
    "# Python equivalent of our SQL hour analysis\n",
    "\n",
    "hourly_orders = orders_df.groupby(\n",
    "    orders_df['order_purchase_timestamp'].dt.hour\n",
    ")['order_id'].count().reset_index()\n",
    "\n",
    "hourly_orders.columns = ['hour_of_day', 'order_count']\n",
    "hourly_orders['percentage'] = (\n",
    "    hourly_orders['order_count'] / hourly_orders['order_count'].sum() * 100\n",
    ").round(2)\n",
    "\n",
    "# Sort by order count to find peak hours\n",
    "hourly_orders_sorted = hourly_orders.sort_values('order_count', ascending=False)\n",
    "\n",
    "print(\"ğŸ• Peak Shopping Hours Analysis:\")\n",
    "print(\"Top 5 shopping hours:\")\n",
    "print(hourly_orders_sorted.head())\n",
    "\n",
    "# Create a simple visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(hourly_orders['hour_of_day'], hourly_orders['order_count'])\n",
    "plt.title('Shopping Patterns by Hour of Day')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Orders')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ¯ Business Insight: Peak shopping hour is {hourly_orders_sorted.iloc[0]['hour_of_day']}:00 with {hourly_orders_sorted.iloc[0]['order_count']} orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Business Insights Summary\n",
    "Let's compare our Python results with Thursday's SQL findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive summary of our temporal analysis\n",
    "print(\"ğŸ“‹ BUSINESS INSIGHTS SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Dataset Overview\n",
    "print(f\"ğŸ“Š Total Orders Analyzed: {len(orders_df):,}\")\n",
    "print(f\"ğŸ“… Date Range: {orders_df['order_purchase_timestamp'].min().strftime('%Y-%m-%d')} to {orders_df['order_purchase_timestamp'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"ğŸ¯ Delivered Orders: {(orders_df['order_status'] == 'delivered').sum():,}\")\n",
    "\n",
    "# 2. Seasonal Patterns\n",
    "print(\"\\nğŸŒ¿ SEASONAL PATTERNS:\")\n",
    "quarterly_summary = orders_df.groupby(\n",
    "    orders_df['order_purchase_timestamp'].dt.quarter\n",
    ")['order_id'].count()\n",
    "peak_quarter = quarterly_summary.idxmax()\n",
    "print(f\"ğŸ“ˆ Peak Quarter: Q{peak_quarter} with {quarterly_summary.max():,} orders\")\n",
    "\n",
    "# 3. Customer Behavior\n",
    "print(\"\\nğŸ‘¥ CUSTOMER BEHAVIOR:\")\n",
    "weekend_orders = day_type_summary.loc['Weekend', 'total_orders']\n",
    "weekday_orders = day_type_summary.loc['Weekday', 'total_orders']\n",
    "weekend_percentage = day_type_summary.loc['Weekend', 'percentage_of_total']\n",
    "print(f\"ğŸ–ï¸ Weekend Shopping: {weekend_orders:,} orders ({weekend_percentage}%)\")\n",
    "print(f\"ğŸ’¼ Weekday Shopping: {weekday_orders:,} orders ({100-weekend_percentage:.1f}%)\")\n",
    "\n",
    "# 4. Peak Hours\n",
    "peak_hour = hourly_orders_sorted.iloc[0]['hour_of_day']\n",
    "peak_hour_orders = hourly_orders_sorted.iloc[0]['order_count']\n",
    "print(f\"â° Peak Shopping Hour: {peak_hour}:00 with {peak_hour_orders:,} orders\")\n",
    "\n",
    "print(\"\\nâœ… Analysis complete! These insights match our SQL findings from Thursday.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Key Takeaways\n",
    "\n",
    "### Python vs SQL for DateTime Operations\n",
    "\n",
    "| Operation | SQL | Python Pandas | Best Use Case |\n",
    "|-----------|-----|---------------|---------------|\n",
    "| Date Extraction | `EXTRACT(MONTH FROM date)` | `df['date'].dt.month` | Both equally effective |\n",
    "| Date Formatting | `TO_CHAR(date, 'Month YYYY')` | `df['date'].dt.strftime('%B %Y')` | Python more flexible |\n",
    "| Time Grouping | `DATE_TRUNC('month', date)` | `df.resample('M')` | Python better for analysis |\n",
    "| Missing Data | `WHERE date IS NOT NULL` | `df['date'].notna()` | Both handle well |\n",
    "\n",
    "### When to Use Each Tool\n",
    "- **SQL**: Great for data extraction, filtering, and database-level aggregations\n",
    "- **Python**: Better for complex analysis, visualizations, and machine learning\n",
    "- **Both**: Can achieve the same business insights with different approaches\n",
    "\n",
    "### Next Steps\n",
    "In Part 2, we'll dive deeper into:\n",
    "- Date arithmetic and timedelta operations\n",
    "- Advanced time series analysis\n",
    "- Rolling windows and trend analysis\n",
    "- Seasonal decomposition techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Practice Exercise\n",
    "Try these quick exercises to reinforce your learning:\n",
    "\n",
    "1. **Extract all orders from December** (any year)\n",
    "2. **Find the busiest day of the week** for orders\n",
    "3. **Calculate the percentage of orders placed on weekends vs weekdays**\n",
    "4. **Create a month-year column** in the format \"Jan 2018\"\n",
    "\n",
    "Try to solve these before looking at the solutions in the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}