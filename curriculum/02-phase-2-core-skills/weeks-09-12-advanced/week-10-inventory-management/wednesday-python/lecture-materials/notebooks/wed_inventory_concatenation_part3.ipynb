{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Wednesday - Part 3: Data Concatenation with Pandas\n",
    "\n",
    "**Duration:** 30 minutes  \n",
    "**Topic:** Combining DataFrames with `pd.concat()`  \n",
    "**Business Context:** Combining Multi-Period Inventory Reports  \n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1. Combine multiple DataFrames vertically (stacking rows)\n",
    "2. Combine multiple DataFrames horizontally (adding columns)\n",
    "3. Understand when to use `concat()` vs `merge()`\n",
    "4. Handle index alignment and duplicate handling\n",
    "5. Apply concatenation to real-world multi-file scenarios\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction: Why Concatenation Matters\n",
    "\n",
    "In real business scenarios, data often comes in multiple files:\n",
    "\n",
    "- **Time-based splits:** Monthly sales reports (jan_sales.csv, feb_sales.csv, mar_sales.csv)\n",
    "- **Regional splits:** Lagos_inventory.csv, Abuja_inventory.csv, PH_inventory.csv\n",
    "- **Category splits:** Electronics_data.csv, Furniture_data.csv, etc.\n",
    "\n",
    "**Concatenation (`pd.concat()`)** allows you to:\n",
    "- Stack these files vertically (combine rows)\n",
    "- Combine them horizontally (add new columns)\n",
    "- Create unified datasets for analysis\n",
    "\n",
    "### Concat vs Merge:\n",
    "\n",
    "| Operation | Use Case | Key Difference |\n",
    "|-----------|----------|----------------|\n",
    "| **merge()** | Combine based on common columns (keys) | Intelligent matching (like SQL JOIN) |\n",
    "| **concat()** | Stack DataFrames together | Simple stacking (no matching logic) |\n",
    "\n",
    "---\n",
    "\n",
    "## Setup: Import Libraries and Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our main datasets\n",
    "products = pd.read_csv('../datasets/products.csv')\n",
    "inventory = pd.read_csv('../datasets/inventory.csv')\n",
    "warehouses = pd.read_csv('../datasets/warehouses.csv')\n",
    "\n",
    "print(\"‚úì Datasets loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Vertical Concatenation - Stacking Rows (12 minutes)\n",
    "\n",
    "**Business Scenario:** You receive monthly inventory snapshots as separate files. You need to combine them into one dataset.\n",
    "\n",
    "### Creating Sample Monthly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate monthly inventory snapshots\n",
    "jan_inventory = inventory.head(7).copy()\n",
    "jan_inventory['snapshot_date'] = '2025-01-31'\n",
    "\n",
    "feb_inventory = inventory.iloc[7:14].copy()\n",
    "feb_inventory['snapshot_date'] = '2025-02-28'\n",
    "\n",
    "mar_inventory = inventory.iloc[14:21].copy() if len(inventory) > 14 else inventory.head(7).copy()\n",
    "mar_inventory['snapshot_date'] = '2025-03-31'\n",
    "\n",
    "print(\"January Snapshot:\")\n",
    "print(jan_inventory.head())\n",
    "print(f\"\\nJanuary records: {len(jan_inventory)}\")\n",
    "print(f\"February records: {len(feb_inventory)}\")\n",
    "print(f\"March records: {len(mar_inventory)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Basic Vertical Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all monthly snapshots into one DataFrame\n",
    "all_months = pd.concat(\n",
    "    [jan_inventory, feb_inventory, mar_inventory],\n",
    "    axis=0,              # axis=0 means vertical stacking (rows)\n",
    "    ignore_index=True    # Create new sequential index\n",
    ")\n",
    "\n",
    "print(f\"Combined inventory records: {len(all_months)}\")\n",
    "print(f\"Expected: {len(jan_inventory) + len(feb_inventory) + len(mar_inventory)}\")\n",
    "print(\"\\nSample of combined data:\")\n",
    "print(all_months.head())\n",
    "print(\"\\nMonths represented:\")\n",
    "print(all_months['snapshot_date'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Insight:** `axis=0` stacks DataFrames vertically, and `ignore_index=True` creates a new continuous index.\n",
    "\n",
    "### Example 2: Keeping Original Index with Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add identifying keys to track source of each row\n",
    "all_months_keyed = pd.concat(\n",
    "    [jan_inventory, feb_inventory, mar_inventory],\n",
    "    axis=0,\n",
    "    keys=['January', 'February', 'March'],  # Add hierarchical index\n",
    "    names=['Month', 'Original_Index']       # Name the index levels\n",
    ")\n",
    "\n",
    "print(\"With hierarchical index:\")\n",
    "print(all_months_keyed.head(10))\n",
    "print(\"\\nAccess January data:\")\n",
    "print(all_months_keyed.loc['January'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Handling Missing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames with different columns (common real-world issue)\n",
    "df1 = pd.DataFrame({\n",
    "    'product_id': ['A1', 'A2', 'A3'],\n",
    "    'stock': [100, 150, 200],\n",
    "    'warehouse': ['Lagos', 'Abuja', 'Lagos']\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'product_id': ['B1', 'B2'],\n",
    "    'stock': [75, 120],\n",
    "    'supplier': ['SupplierX', 'SupplierY']  # Different column!\n",
    "})\n",
    "\n",
    "print(\"DataFrame 1:\")\n",
    "print(df1)\n",
    "print(\"\\nDataFrame 2:\")\n",
    "print(df2)\n",
    "\n",
    "# Concatenate with outer join (default) - keeps all columns\n",
    "combined_outer = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "print(\"\\nCombined (outer join - keeps all columns):\")\n",
    "print(combined_outer)\n",
    "print(\"\\n‚ö†Ô∏è Notice the NaN values where columns don't match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate with inner join - only common columns\n",
    "combined_inner = pd.concat([df1, df2], axis=0, join='inner', ignore_index=True)\n",
    "print(\"Combined (inner join - only common columns):\")\n",
    "print(combined_inner)\n",
    "print(\"\\n‚úì Only 'product_id' and 'stock' columns kept\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Insight:** Use `join='inner'` to keep only common columns, or `join='outer'` (default) to keep all columns with NaN for missing values.\n",
    "\n",
    "---\n",
    "\n",
    "## Section 2: Horizontal Concatenation - Adding Columns (8 minutes)\n",
    "\n",
    "**Business Scenario:** You have product information in one file and supplier ratings in another. You want to add supplier data as new columns.\n",
    "\n",
    "### Example 4: Basic Horizontal Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with aligned indices\n",
    "product_info = pd.DataFrame({\n",
    "    'product_id': ['P1', 'P2', 'P3', 'P4'],\n",
    "    'category': ['Electronics', 'Furniture', 'Electronics', 'Home']\n",
    "})\n",
    "\n",
    "price_info = pd.DataFrame({\n",
    "    'price': [25000, 45000, 15000, 8000],\n",
    "    'currency': ['‚Ç¶', '‚Ç¶', '‚Ç¶', '‚Ç¶']\n",
    "})\n",
    "\n",
    "print(\"Product Info:\")\n",
    "print(product_info)\n",
    "print(\"\\nPrice Info:\")\n",
    "print(price_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine horizontally (add columns)\n",
    "complete_product = pd.concat(\n",
    "    [product_info, price_info],\n",
    "    axis=1  # axis=1 means horizontal concatenation (columns)\n",
    ")\n",
    "\n",
    "print(\"Combined (horizontal):\")\n",
    "print(complete_product)\n",
    "print(f\"\\nColumns increased from {len(product_info.columns)} + {len(price_info.columns)} = {len(complete_product.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ö†Ô∏è Important:** Horizontal concatenation aligns on the index. If indices don't match, you'll get NaN values.\n",
    "\n",
    "### Example 5: Index Alignment Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames with mismatched indices\n",
    "df_a = pd.DataFrame({\n",
    "    'A': [1, 2, 3]\n",
    "}, index=[0, 1, 2])\n",
    "\n",
    "df_b = pd.DataFrame({\n",
    "    'B': [4, 5, 6]\n",
    "}, index=[1, 2, 3])  # Different indices!\n",
    "\n",
    "print(\"DataFrame A (index 0,1,2):\")\n",
    "print(df_a)\n",
    "print(\"\\nDataFrame B (index 1,2,3):\")\n",
    "print(df_b)\n",
    "\n",
    "# Concatenate horizontally\n",
    "result = pd.concat([df_a, df_b], axis=1)\n",
    "print(\"\\nConcatenated Result:\")\n",
    "print(result)\n",
    "print(\"\\n‚ö†Ô∏è Notice NaN values where indices don't align!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Solution:** For horizontal concat, consider using `merge()` instead if you need intelligent matching:\n",
    "\n",
    "```python\n",
    "# Better approach for misaligned data:\n",
    "result = pd.merge(df_a, df_b, left_index=True, right_index=True, how='outer')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Section 3: Real-World Use Cases (8 minutes)\n",
    "\n",
    "### Use Case 1: Combining Regional Inventory Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate regional inventory files\n",
    "lagos_warehouse = warehouses[warehouses['city'] == 'Lagos']\n",
    "lagos_inventory = inventory[inventory['warehouse_id'].isin(lagos_warehouse['warehouse_id'])].copy()\n",
    "lagos_inventory['region'] = 'Lagos'\n",
    "\n",
    "abuja_warehouse = warehouses[warehouses['city'] == 'Abuja']\n",
    "abuja_inventory = inventory[inventory['warehouse_id'].isin(abuja_warehouse['warehouse_id'])].copy()\n",
    "abuja_inventory['region'] = 'Abuja'\n",
    "\n",
    "print(f\"Lagos inventory: {len(lagos_inventory)} records\")\n",
    "print(f\"Abuja inventory: {len(abuja_inventory)} records\")\n",
    "\n",
    "# Combine regional files\n",
    "national_inventory = pd.concat(\n",
    "    [lagos_inventory, abuja_inventory],\n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(f\"\\nNational inventory: {len(national_inventory)} records\")\n",
    "print(\"\\nRegional distribution:\")\n",
    "print(national_inventory['region'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 2: Appending New Data to Existing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing inventory\n",
    "existing = inventory.head(10).copy()\n",
    "print(f\"Existing records: {len(existing)}\")\n",
    "print(f\"Last product_id: {existing['product_id'].iloc[-1]}\")\n",
    "\n",
    "# New incoming data (e.g., from daily update)\n",
    "new_data = pd.DataFrame({\n",
    "    'product_id': ['NEW001', 'NEW002'],\n",
    "    'warehouse_id': [1, 2],\n",
    "    'stock_level': [50, 75],\n",
    "    'reorder_point': [10, 15],\n",
    "    'last_restocked': ['2025-10-26', '2025-10-26'],\n",
    "    'status': ['In Stock', 'In Stock']\n",
    "})\n",
    "\n",
    "print(\"\\nNew incoming data:\")\n",
    "print(new_data)\n",
    "\n",
    "# Append new data\n",
    "updated_inventory = pd.concat([existing, new_data], axis=0, ignore_index=True)\n",
    "print(f\"\\nUpdated inventory: {len(updated_inventory)} records\")\n",
    "print(\"\\nLast 3 records (including new):\")\n",
    "print(updated_inventory.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case 3: Combining Data from Multiple File Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realistic scenario: You have data in different formats\n",
    "# CSV file data\n",
    "csv_data = inventory.head(5)[['product_id', 'warehouse_id', 'stock_level']].copy()\n",
    "csv_data['source'] = 'CSV'\n",
    "\n",
    "# Excel file data (simulated)\n",
    "excel_data = pd.DataFrame({\n",
    "    'product_id': ['E1', 'E2', 'E3'],\n",
    "    'warehouse_id': [1, 2, 3],\n",
    "    'stock_level': [120, 95, 80],\n",
    "    'source': ['Excel', 'Excel', 'Excel']\n",
    "})\n",
    "\n",
    "# Database query data (simulated)\n",
    "db_data = pd.DataFrame({\n",
    "    'product_id': ['D1', 'D2'],\n",
    "    'warehouse_id': [1, 1],\n",
    "    'stock_level': [200, 150],\n",
    "    'source': ['Database', 'Database']\n",
    "})\n",
    "\n",
    "# Combine all sources\n",
    "consolidated = pd.concat(\n",
    "    [csv_data, excel_data, db_data],\n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(\"Consolidated inventory from multiple sources:\")\n",
    "print(consolidated)\n",
    "print(\"\\nData source breakdown:\")\n",
    "print(consolidated['source'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Best Practices and Common Pitfalls (2 minutes)\n",
    "\n",
    "### Decision Tree: Concat vs Merge\n",
    "\n",
    "```\n",
    "Do you need to match records based on common values (keys)?\n",
    "‚îú‚îÄ YES ‚Üí Use merge() or join()\n",
    "‚îÇ   ‚îî‚îÄ Example: Combine products with their suppliers\n",
    "‚îÇ\n",
    "‚îî‚îÄ NO ‚Üí Use concat()\n",
    "    ‚îú‚îÄ Same structure, different time periods?\n",
    "    ‚îÇ   ‚îî‚îÄ concat(axis=0) [vertical stacking]\n",
    "    ‚îÇ\n",
    "    ‚îî‚îÄ Different columns, same rows?\n",
    "        ‚îî‚îÄ concat(axis=1) [horizontal stacking]\n",
    "```\n",
    "\n",
    "### Quick Reference Table\n",
    "\n",
    "| Scenario | Method | Parameters |\n",
    "|----------|--------|------------|\n",
    "| Stack monthly files | `pd.concat()` | `axis=0, ignore_index=True` |\n",
    "| Combine regional data | `pd.concat()` | `axis=0, keys=['Region1', ...]` |\n",
    "| Add new columns | `pd.concat()` | `axis=1` |\n",
    "| Combine with matching | `pd.merge()` | `on='key_column'` |\n",
    "| Only common columns | `pd.concat()` | `axis=0, join='inner'` |\n",
    "| All columns | `pd.concat()` | `axis=0, join='outer'` |\n",
    "\n",
    "### Common Pitfalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall 1: Forgetting to reset index\n",
    "bad_concat = pd.concat([jan_inventory, feb_inventory], axis=0)\n",
    "print(\"Without ignore_index (duplicate indices):\")\n",
    "print(bad_concat.head(10))\n",
    "print(f\"\\nIndex has duplicates: {bad_concat.index.duplicated().any()}\")\n",
    "\n",
    "# Pitfall 2: Wrong axis\n",
    "print(\"\\n‚ùå Using axis=1 when you meant axis=0:\")\n",
    "wrong_axis = pd.concat([jan_inventory.head(3), feb_inventory.head(3)], axis=1)\n",
    "print(f\"Shape: {wrong_axis.shape} (way too many columns!)\")\n",
    "\n",
    "# Pitfall 3: Not tracking data source\n",
    "print(\"\\n‚ö†Ô∏è No way to identify which month each record came from!\")\n",
    "print(\"‚úì Solution: Add 'source' column or use keys parameter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Vertical Concatenation (`axis=0`):**\n",
    "   - Stacks DataFrames on top of each other\n",
    "   - Use `ignore_index=True` for new sequential index\n",
    "   - Use `keys` parameter to track source\n",
    "   - Use `join='inner'` for only common columns\n",
    "\n",
    "2. **Horizontal Concatenation (`axis=1`):**\n",
    "   - Adds columns side-by-side\n",
    "   - Aligns on index (be careful with mismatches!)\n",
    "   - Consider using `merge()` if indices don't align\n",
    "\n",
    "3. **Concat vs Merge:**\n",
    "   - **concat()**: Simple stacking, no matching logic\n",
    "   - **merge()**: Intelligent matching based on keys (like SQL JOIN)\n",
    "\n",
    "### Quick Syntax Reference:\n",
    "\n",
    "```python\n",
    "# Vertical stacking (common use case)\n",
    "pd.concat([df1, df2, df3], axis=0, ignore_index=True)\n",
    "\n",
    "# With source tracking\n",
    "pd.concat([df1, df2], axis=0, keys=['Source1', 'Source2'])\n",
    "\n",
    "# Horizontal (add columns)\n",
    "pd.concat([df1, df2], axis=1)\n",
    "\n",
    "# Only common columns\n",
    "pd.concat([df1, df2], axis=0, join='inner')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Practice Exercise (5 minutes)\n",
    "\n",
    "**Challenge:** \n",
    "1. Create three separate DataFrames representing Q1, Q2, and Q3 product sales\n",
    "2. Add a 'quarter' column to each\n",
    "3. Combine them into a single annual report\n",
    "4. Calculate total sales by quarter\n",
    "\n",
    "### Your Task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create Q1, Q2, Q3 DataFrames\n",
    "# Step 2: Add quarter identifier\n",
    "# Step 3: Concatenate\n",
    "# Step 4: Analyze\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution (Reveal After Attempting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "q1_sales = pd.DataFrame({\n",
    "    'product': ['A', 'B', 'C'],\n",
    "    'sales': [10000, 15000, 12000],\n",
    "    'quarter': ['Q1', 'Q1', 'Q1']\n",
    "})\n",
    "\n",
    "q2_sales = pd.DataFrame({\n",
    "    'product': ['A', 'B', 'C'],\n",
    "    'sales': [12000, 16000, 13000],\n",
    "    'quarter': ['Q2', 'Q2', 'Q2']\n",
    "})\n",
    "\n",
    "q3_sales = pd.DataFrame({\n",
    "    'product': ['A', 'B', 'C'],\n",
    "    'sales': [15000, 18000, 14000],\n",
    "    'quarter': ['Q3', 'Q3', 'Q3']\n",
    "})\n",
    "\n",
    "annual_report = pd.concat([q1_sales, q2_sales, q3_sales], axis=0, ignore_index=True)\n",
    "\n",
    "print(\"Annual Sales Report:\")\n",
    "print(annual_report)\n",
    "\n",
    "print(\"\\nTotal Sales by Quarter:\")\n",
    "print(annual_report.groupby('quarter')['sales'].sum())\n",
    "\n",
    "print(\"\\nTotal Sales by Product:\")\n",
    "print(annual_report.groupby('product')['sales'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Session Wrap-Up\n",
    "\n",
    "### Today's Complete Learning Journey:\n",
    "\n",
    "**Part 1 - Merging (45 min):**\n",
    "- Inner, left, right, outer joins\n",
    "- Combining related datasets based on keys\n",
    "\n",
    "**Part 2 - Reshaping (45 min):**\n",
    "- Pivot tables (long ‚Üí wide)\n",
    "- Melt (wide ‚Üí long)\n",
    "- Stack/Unstack operations\n",
    "\n",
    "**Part 3 - Concatenation (30 min - Today):**\n",
    "- Vertical stacking (combining rows)\n",
    "- Horizontal stacking (adding columns)\n",
    "- Real-world multi-file scenarios\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**Practice Exercises** (Remaining class time):\n",
    "- Complete comprehensive exercises notebook\n",
    "- Apply all three techniques to inventory analysis\n",
    "- Prepare questions for Q&A\n",
    "\n",
    "**Thursday SQL Session:**\n",
    "- Database normalization concepts\n",
    "- Creating views and materialized views\n",
    "- Data integrity and constraints\n",
    "- See how today's pandas concepts map to SQL design\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Pandas concat() documentation](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)\n",
    "- [Merge, join, concatenate guide](https://pandas.pydata.org/docs/user_guide/merging.html)\n",
    "- [Comparison: concat vs merge vs join](https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html#join)\n",
    "- Week 10 Resources folder: Cheat sheets and quick reference guides"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
